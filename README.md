<div align="center">

# ğŸ¤– Mini RAG App

### *Your Documents, Supercharged with AI* âœ¨

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-purple.svg)](LICENSE)

**Upload. Process. Ask. Get Answers.** ğŸš€

Transform your documents into an intelligent Q&A system powered by RAG (Retrieval Augmented Generation)

[Quick Start](#-quick-start) â€¢ [What is RAG?](#-understanding-rag-retrieval-augmented-generation) â€¢ [API Docs](#-api-playground) â€¢ [Examples](#-real-examples)

</div>

---

## ğŸ“– What is Mini RAG App?

**Mini RAG App** is a production-ready, **scalable** implementation of a **Retrieval Augmented Generation (RAG)** system built with modern Python technologies. It transforms your documents into an intelligent knowledge base that can answer questions with accuracy and context.

### ğŸ¯ The Problem It Solves

Traditional chatbots and LLMs have a critical limitation: they can only answer based on their training data, which means:
- âŒ No knowledge of YOUR specific documents
- âŒ Can't access proprietary or recent information
- âŒ Prone to "hallucinations" (making up answers)

**Mini RAG App solves this** by grounding AI responses in your actual documents, ensuring accurate, verifiable answers.

### ğŸ”Œ Scalable Multi-Provider Architecture

This project is built with **flexibility and scalability** in mind. Thanks to the **Factory Pattern** design, you can seamlessly switch between multiple LLM providers without changing your code:

<table>
<tr>
<td align="center" width="33%">

### â˜ï¸ **OpenAI**
**Cloud-based, powerful**

- GPT-3.5/GPT-4 for generation
- text-embedding-ada-002
- Best for production
- Pay-per-use pricing

</td>
<td align="center" width="33%">

### ğŸ”® **Cohere**
**Alternative cloud option**

- Command models
- Cohere embeddings
- Competitive pricing
- Great multilingual support

</td>
<td align="center" width="33%">

### ğŸ¦™ **Ollama**
**100% Local & Free**

- Runs on your machine
- No API costs
- Complete privacy
- Perfect for development

</td>
</tr>
</table>

**Switching providers?** Just change a few lines in your `.env` file. The application automatically adapts! ğŸ‰

```env
# Switch from OpenAI to Ollama? Just update these:
GENERATION_BACKEND="OPENAI"  # or "COHERE"
EMBEDDING_BACKEND="OPENAI"   # or "COHERE"
```

The **Factory Pattern** implementation means:
- âœ… **Zero code changes** when switching providers
- âœ… **Easy to add new providers** (just implement the interface)
- âœ… **Test with Ollama locally**, deploy with OpenAI in production
- âœ… **Mix and match**: Use OpenAI for generation, Cohere for embeddings

---

## ğŸ§  Understanding RAG (Retrieval Augmented Generation)

### What is RAG?

**RAG** combines the power of **information retrieval** with **generative AI** to create a system that:
1. **Retrieves** relevant information from your documents
2. **Augments** the AI prompt with this context
3. **Generates** accurate answers based on retrieved facts

### The RAG Pipeline Explained

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG WORKFLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ï¿½ STEP 1: DOCUMENT INGESTION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Your Documentâ”‚  (PDF, TXT)
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Text Splitterâ”‚  Split into chunks (e.g., 500 chars with 50 overlap)
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Chunks: ["chunk1", "chunk2", ...] â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§® STEP 2: EMBEDDING & INDEXING
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Each Chunk   â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ LLM Embeddingâ”‚  Convert text â†’ vector (e.g., [0.23, -0.45, ...])
   â”‚   Model      â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Qdrant Vectorâ”‚  Store vectors for fast similarity search
   â”‚   Database   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ” STEP 3: QUERY & RETRIEVAL (When user asks a question)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ User Question:       â”‚
   â”‚ "What is the main    â”‚
   â”‚  topic?"             â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Embed Query  â”‚  Convert question â†’ vector
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Vector Searchâ”‚  Find most similar chunks (cosine similarity)
   â”‚  in Qdrant   â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Top-K Relevant Chunks      â”‚
   â”‚ ["chunk 5", "chunk 12", ...]â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ’¬ STEP 4: GENERATION (RAG Magic!)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Prompt Template:                    â”‚
   â”‚                                     â”‚
   â”‚ Context: [Retrieved chunks]         â”‚
   â”‚ Question: [User question]           â”‚
   â”‚                                     â”‚
   â”‚ Answer based ONLY on the context.  â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ LLM (GPT/    â”‚  Generate answer grounded in context
   â”‚  Cohere/     â”‚
   â”‚  Ollama)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ âœ… Accurate Answer               â”‚
   â”‚ "The main topic is..."           â”‚
   â”‚ (Based on YOUR documents!)       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why RAG is Powerful

| Traditional LLM | RAG-Enhanced LLM |
|----------------|------------------|
| âŒ Limited to training data | âœ… Uses YOUR documents |
| âŒ Can't access new info | âœ… Always up-to-date |
| âŒ Hallucinates answers | âœ… Grounded in facts |
| âŒ No source attribution | âœ… Can cite sources |
| âŒ Generic responses | âœ… Domain-specific answers |

### Real-World Example

**Without RAG:**
```
User: "What was our Q3 revenue?"
LLM: "I don't have access to your company's financial data."
```

**With RAG:**
```
User: "What was our Q3 revenue?"
RAG System:
  1. Searches your uploaded financial reports
  2. Finds: "Q3 2024 revenue reached $2.5M..."
  3. LLM generates: "According to your Q3 report, revenue was $2.5M,
     representing a 15% increase from Q2."
```

---

## ğŸ¯ What Can It Do?

<table>
<tr>
<td width="50%">

### ï¿½ğŸ“¤ **Smart Document Processing**
- Drop in your PDFs or text files
- Auto-chunking with intelligent overlap
- Metadata extraction & organization

</td>
<td width="50%">

### ğŸ§  **AI-Powered Search**
- Semantic search (not just keywords!)
- Vector embeddings via OpenAI/Cohere/Ollama
- Lightning-fast Qdrant vector DB

</td>
</tr>
<tr>
<td width="50%">

### ğŸ’¬ **RAG Question Answering**
- Ask questions in natural language
- Get answers grounded in YOUR docs
- No hallucinations, just facts

</td>
<td width="50%">

### ğŸ¨ **Multi-Project Support**
- Organize docs into projects
- Isolated knowledge bases
- Easy project switching

</td>
</tr>
</table>

---

## âš¡ Quick Start

### 1ï¸âƒ£ **Clone & Install**

```bash
# Clone the repo
git clone <your-repo-url>
cd mini-rag-app

# Create virtual environment
python -m venv .venv

# Activate it
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Mac/Linux

# Install dependencies
pip install -r src/requirements.txt
```

### 2ï¸âƒ£ **Setup MongoDB**

<details>
<summary><b>ğŸ³ Option A: Docker (Recommended)</b></summary>

```bash
cd docker
docker-compose up -d
```

That's it! MongoDB will be running on `localhost:27017` ğŸ‰

</details>

<details>
<summary><b>ğŸ’» Option B: Local MongoDB</b></summary>

**Windows:**
1. Download from [MongoDB Download Center](https://www.mongodb.com/try/download/community)
2. Install with default settings
3. MongoDB will auto-start as a service

**Mac:**
```bash
brew tap mongodb/brew
brew install mongodb-community
brew services start mongodb-community
```

**Linux:**
```bash
sudo apt-get install -y mongodb-org
sudo systemctl start mongod
```

Verify it's running:
```bash
mongosh --eval "db.version()"
```

</details>

### 3ï¸âƒ£ **Configure Your LLM**

```bash
cd src
cp .env.example .env
```

Now edit `.env` and choose your AI provider:

<details>
<summary><b>ğŸŒ OpenAI (Cloud)</b></summary>

```env
GENERATION_BACKEND="OPENAI"
EMBEDDING_BACKEND="OPENAI"
OPENAI_API_KEY="sk-your-actual-key-here"
OPENAI_API_URL="https://api.openai.com/v1/"
GENERATION_MODEL_ID="gpt-3.5-turbo"
EMBEDDING_MODEL_ID="text-embedding-ada-002"
EMBEDDING_MODEL_SIZE=1536
```

</details>

<details>
<summary><b>ğŸ¦™ Ollama (Local & Free!)</b></summary>

First, install Ollama from [ollama.ai](https://ollama.ai), then:

```bash
# Pull the models
ollama pull qwen2.5-coder:7b
ollama pull nomic-embed-text
```

Update `.env`:
```env
GENERATION_BACKEND="OPENAI"
EMBEDDING_BACKEND="OPENAI"
OPENAI_API_KEY="not-needed"
OPENAI_API_URL="http://localhost:11434/v1/"
GENERATION_MODEL_ID="qwen2.5-coder:7b"
EMBEDDING_MODEL_ID="nomic-embed-text"
EMBEDDING_MODEL_SIZE=768
```

</details>

<details>
<summary><b>ğŸ”® Cohere</b></summary>

```env
GENERATION_BACKEND="COHERE"
EMBEDDING_BACKEND="COHERE"
COHERE_API_KEY="your-cohere-key"
```

</details>

### 4ï¸âƒ£ **Launch! ğŸš€**

```bash
cd src
uvicorn main:app --reload --port 8000
```

**ğŸ‰ Done!** Visit `http://localhost:8000/docs` for the interactive API playground.

---

## ğŸ® Real Examples

### Example 1: Upload & Query a Document

```bash
# 1. Upload your document
curl -X POST "http://localhost:8000/api/v1/data/upload/my_project" \
  -F "file=@research_paper.pdf"

# Response: {"signal": "file_upload_success", "file_id": "abc123"}

# 2. Process it into chunks
curl -X POST "http://localhost:8000/api/v1/data/process/my_project" \
  -H "Content-Type: application/json" \
  -d '{
    "file_id": "abc123",
    "chunk_size": 500,
    "overlap_size": 50,
    "do_reset": 0
  }'

# Response: {"signal": "processing_success", "inserted_chunks": 42}

# 3. Index into vector database
curl -X POST "http://localhost:8000/api/v1/nlp/index/push/my_project" \
  -H "Content-Type: application/json" \
  -d '{"do_reset": 0}'

# Response: {"signal": "insert_into_vectordb_success", "inserted_items_count": 42}

# 4. Ask a question!
curl -X POST "http://localhost:8000/api/v1/nlp/index/answer/my_project" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "What are the main findings?",
    "limit": 5
  }'

# Response: {"signal": "rag_answer_success", "answer": "The main findings are..."}
```

### Example 2: Search for Similar Content

```bash
curl -X POST "http://localhost:8000/api/v1/nlp/index/search/my_project" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "machine learning algorithms",
    "limit": 3
  }'
```

Returns the top 3 most relevant chunks from your documents!

---

## ğŸ“š API Playground

Once running, explore the **auto-generated interactive docs**:

- **Swagger UI**: http://localhost:8000/docs ğŸ‘ˆ *Try it live!*
- **ReDoc**: http://localhost:8000/redoc ğŸ‘ˆ *Beautiful docs*

### Core Endpoints

| Endpoint | Method | What It Does |
|----------|--------|--------------|
| `/api/v1/data/upload/{project_id}` | POST | ğŸ“¤ Upload a file |
| `/api/v1/data/process/{project_id}` | POST | âœ‚ï¸ Chunk the document |
| `/api/v1/nlp/index/push/{project_id}` | POST | ğŸ—„ï¸ Index into vector DB |
| `/api/v1/nlp/index/search/{project_id}` | POST | ğŸ” Semantic search |
| `/api/v1/nlp/index/answer/{project_id}` | POST | ğŸ’¬ RAG Q&A |
| `/api/v1/nlp/index/info/{project_id}` | GET | â„¹ï¸ Get index stats |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI Server                      â”‚
â”‚                  (Async, High Performance)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚            â”‚
        â–¼            â–¼            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Upload â”‚  â”‚ Process â”‚  â”‚   RAG    â”‚
   â”‚  API   â”‚  â”‚   API   â”‚  â”‚   API    â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚            â”‚            â”‚
       â–¼            â–¼            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚         Controllers Layer          â”‚
   â”‚  (Business Logic & Orchestration)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚           â”‚           â”‚
       â–¼           â–¼           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚MongoDB â”‚ â”‚Qdrant  â”‚ â”‚   LLM    â”‚
   â”‚(Chunks)â”‚ â”‚(Vectors)â”‚ â”‚(OpenAI/  â”‚
   â”‚        â”‚ â”‚        â”‚ â”‚ Cohere/  â”‚
   â”‚        â”‚ â”‚        â”‚ â”‚ Ollama)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ Project Structure

```
mini-rag-app/
â”‚
â”œâ”€â”€ ğŸš€ src/
â”‚   â”œâ”€â”€ main.py                    # FastAPI app entry point
â”‚   â”œâ”€â”€ .env.example               # Config template
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ›£ï¸ routes/                 # API endpoints
â”‚   â”‚   â”œâ”€â”€ data.py                # Upload & processing
â”‚   â”‚   â”œâ”€â”€ nlp.py                 # Search & RAG
â”‚   â”‚   â””â”€â”€ schemes/               # Request/response models
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ® controllers/            # Business logic
â”‚   â”‚   â”œâ”€â”€ DataController.py      # File validation
â”‚   â”‚   â”œâ”€â”€ ProcessController.py   # Document chunking
â”‚   â”‚   â”œâ”€â”€ NLPController.py       # RAG operations
â”‚   â”‚   â””â”€â”€ ProjectController.py   # Project management
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ’¾ models/                 # Database models
â”‚   â”‚   â”œâ”€â”€ ProjectModel.py
â”‚   â”‚   â”œâ”€â”€ AssetModel.py
â”‚   â”‚   â”œâ”€â”€ ChunkModel.py
â”‚   â”‚   â””â”€â”€ db_schemes.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ”Œ stores/                 # External integrations
â”‚   â”‚   â”œâ”€â”€ llm/                   # LLM providers (Factory Pattern)
â”‚   â”‚   â”‚   â”œâ”€â”€ LLMProviderFactory.py
â”‚   â”‚   â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ OpenAIProvider.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ CoHereProvider.py
â”‚   â”‚   â”‚   â””â”€â”€ templates/         # Prompt templates
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ vectordb/              # Vector DB (Factory Pattern)
â”‚   â”‚       â”œâ”€â”€ VectorDBProviderFactory.py
â”‚   â”‚       â””â”€â”€ providers/
â”‚   â”‚           â””â”€â”€ QdrantDBProvider.py
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“¦ assets/                 # Uploaded files
â”‚
â”œâ”€â”€ ğŸ³ docker/
â”‚   â””â”€â”€ docker-compose.yml         # MongoDB setup
â”‚
â””â”€â”€ ğŸ“„ requirements.txt            # Python packages
```

---

## ğŸ”§ Configuration Deep Dive

### Essential Settings

```env
# App Basics
APP_NAME="mini-rag"
FILE_ALLOWED_TYPES=["text/plain","application/pdf"]
FILE_MAX_SIZE=10  # MB

# Database
MONGODB_URL="mongodb://localhost:27017"
MONGODB_DATABASE="mini_rag_db"

# Vector Database
VECTOR_DB_BACKEND="QDRANT"
VECTOR_DB_PATH="qdrant_db"
VECTOR_DB_DISTANCE_METHOD="cosine"

# Generation Settings
GENERATION_DEFAULT_MAX_TOKENS=200
GENERATION_DEFAULT_TEMPERATURE=0.1
INPUT_DEFAULT_MAX_CHARACTERS=1024
```

---

## ğŸ› Troubleshooting

<details>
<summary><b>âŒ "Connection refused to MongoDB"</b></summary>

**Check if MongoDB is running:**
```bash
# Docker
docker ps | grep mongodb

# Local
mongosh --eval "db.version()"
```

**Fix:**
```bash
# Docker
cd docker && docker-compose up -d

# Local
sudo systemctl start mongod  # Linux
brew services start mongodb-community  # Mac
```

</details>

<details>
<summary><b>âŒ "Collection not found" in Qdrant</b></summary>

You forgot to index! Run:
```bash
# 1. Process documents first
POST /api/v1/data/process/{project_id}

# 2. Then index them
POST /api/v1/nlp/index/push/{project_id}
```

</details>

<details>
<summary><b>âŒ "processing_failed" error</b></summary>

**Common causes:**
- Empty file uploaded
- Unsupported file type
- File too large (check `FILE_MAX_SIZE` in `.env`)

**Debug:**
```bash
# Check file size
ls -lh path/to/file

# Check file type
file path/to/file
```

</details>

<details>
<summary><b>âŒ OpenAI/Cohere API errors</b></summary>

**Checklist:**
- âœ… API key is correct in `.env`
- âœ… You have API credits
- âœ… API URL is correct
- âœ… Model ID exists

**For Ollama users:**
```bash
# Make sure Ollama is running
ollama serve

# Check available models
ollama list
```

</details>

<details>
<summary><b>âŒ Module import errors</b></summary>

```bash
# Reinstall dependencies
pip install -r src/requirements.txt --force-reinstall

# Or use a fresh venv
deactivate
rm -rf .venv
python -m venv .venv
.venv\Scripts\activate  # Windows
pip install -r src/requirements.txt
```

</details>

---

## ğŸš€ Advanced Usage

### Batch Process All Files in a Project

```bash
# Don't specify file_id to process ALL files
curl -X POST "http://localhost:8000/api/v1/data/process/my_project" \
  -H "Content-Type: application/json" \
  -d '{
    "chunk_size": 500,
    "overlap_size": 50,
    "do_reset": 1
  }'
```

### Reset & Re-index

```bash
# Clear existing chunks and re-process
curl -X POST "http://localhost:8000/api/v1/data/process/my_project" \
  -d '{"do_reset": 1, "chunk_size": 500, "overlap_size": 50}'

# Clear vector DB and re-index
curl -X POST "http://localhost:8000/api/v1/nlp/index/push/my_project" \
  -d '{"do_reset": 1}'
```

### Check Index Statistics

```bash
curl -X GET "http://localhost:8000/api/v1/nlp/index/info/my_project"
```

---

## ğŸ¤ Contributing

We love contributions! Here's how:

1. **Fork** the repo
2. **Create** a feature branch: `git checkout -b feature/awesome-feature`
3. **Commit** your changes: `git commit -m 'Add awesome feature'`
4. **Push** to the branch: `git push origin feature/awesome-feature`
5. **Open** a Pull Request

---

## ğŸ“œ License

MIT License - feel free to use this in your projects!

---

## ğŸŒŸ Star Us!

If you find this useful, give it a â­ on GitHub!

---

<div align="center">

**Built with â¤ï¸ using FastAPI, LangChain, Qdrant, and MongoDB**

[Report Bug](../../issues) â€¢ [Request Feature](../../issues) â€¢ [Documentation](http://localhost:8000/docs)

</div>